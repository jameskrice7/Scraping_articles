{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scholarly\n",
    "import requests\n",
    "import scholarly\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import requests\n",
    "from newspaper import Article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in Google Scholar search for term climate change: 'dict' object has no attribute 'bib'\n",
      "Error in Google Scholar search for term global warming: 'dict' object has no attribute 'bib'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly\n",
    "\n",
    "class AcademicScraper:\n",
    "    def __init__(self, search_terms=['climate change', 'global warming']):\n",
    "        self.search_terms = search_terms\n",
    "\n",
    "    def scrape_google_scholar(self, year_range=(2010, 2024)):\n",
    "        publications = []\n",
    "        for term in self.search_terms:\n",
    "            try:\n",
    "                search_query = scholarly.search_pubs(term)\n",
    "                for pub in search_query:\n",
    "                    pub = scholarly.fill(pub)\n",
    "                    pub_year = pub.bib.get('pub_year', 0)\n",
    "                    try:\n",
    "                        pub_year = int(pub_year)\n",
    "                    except (ValueError, TypeError):\n",
    "                        pub_year = 0\n",
    "                    if year_range[0] <= pub_year <= year_range[1]:\n",
    "                        publications.append({\n",
    "                            'title': pub.bib.get('title', ''),\n",
    "                            'author': pub.bib.get('author', ''),\n",
    "                            'year': pub_year,\n",
    "                            'text': pub.bib.get('abstract', ''),\n",
    "                            'source': 'Google Scholar'\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Google Scholar search for term {term}: {e}\")\n",
    "        \n",
    "        return publications\n",
    "\n",
    "    def scrape_arxiv(self, year_range=(2010, 2024)):\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        publications = []\n",
    "        \n",
    "        for term in self.search_terms:\n",
    "            query = f'search_query=all:{term}&start=0&max_results=100'\n",
    "            response = requests.get(base_url + query)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                for entry in soup.find_all('entry'):\n",
    "                    published_year = int(entry.published.text[:4])\n",
    "                    if year_range[0] <= published_year <= year_range[1]:\n",
    "                        publications.append({\n",
    "                            'title': entry.title.text,\n",
    "                            'author': ', '.join([au.text for au in entry.find_all('author')]),\n",
    "                            'year': published_year,\n",
    "                            'text': entry.summary.text,\n",
    "                            'source': 'arXiv'\n",
    "                        })\n",
    "        return publications\n",
    "\n",
    "    def collect_academic_corpus(self, year_range=(2010, 2024)):\n",
    "        corpus = []\n",
    "        \n",
    "        # Directly call scraping methods\n",
    "        scholar_pubs = self.scrape_google_scholar(year_range)\n",
    "        arxiv_pubs = self.scrape_arxiv(year_range)\n",
    "        \n",
    "        # Combine and convert to DataFrame\n",
    "        corpus.extend(scholar_pubs)\n",
    "        corpus.extend(arxiv_pubs)\n",
    "        \n",
    "        return pd.DataFrame(corpus)\n",
    "\n",
    "class MediaScraper:\n",
    "    def __init__(self, sources=None):\n",
    "        self.sources = sources or {\n",
    "            'news_outlets': [\n",
    "                'https://www.nytimes.com/section/climate',\n",
    "                'https://www.theguardian.com/environment/climate-change',\n",
    "                'https://www.reuters.com/sustainability/',\n",
    "                'https://www.bbc.com/news/science-environment'\n",
    "            ],\n",
    "            'science_media': [\n",
    "                'https://www.nature.com/climate-change',\n",
    "                'https://www.scientificamerican.com/environment/',\n",
    "                'https://www.newscientist.com/subject/environment/'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def scrape_articles(self, year_range):\n",
    "        # Placeholder implementation as scraping news sites requires proper parsing and rate limiting\n",
    "        # This function should be implemented using requests, BeautifulSoup, or newspaper3k where applicable\n",
    "        # Right now, returning an empty list as a placeholder\n",
    "        return pd.DataFrame(columns=['title', 'author', 'year', 'text', 'source'])\n",
    "\n",
    "class ClimateArticleCorpus:\n",
    "    def __init__(self, year_range=(2015, 2024)):\n",
    "        self.year_range = year_range\n",
    "        self.academic_scraper = AcademicScraper()\n",
    "        self.media_scraper = MediaScraper()\n",
    "    \n",
    "    def collect_corpus(self):\n",
    "        # Collect academic and journalistic sources\n",
    "        academic_df = self.academic_scraper.collect_academic_corpus(self.year_range)\n",
    "        media_df = self.media_scraper.scrape_articles(self.year_range)\n",
    "        \n",
    "        # Combine and clean\n",
    "        combined_df = pd.concat([academic_df, media_df], ignore_index=True)\n",
    "        \n",
    "        # Ensure all text columns are strings and handle NaN values\n",
    "        combined_df['text'] = combined_df['text'].fillna('').astype(str)\n",
    "        \n",
    "        # Additional preprocessing\n",
    "        combined_df['word_count'] = combined_df['text'].apply(lambda x: len(str(x).split()))\n",
    "        combined_df['contains_misinformation_indicators'] = combined_df['text'].apply(\n",
    "            self.detect_potential_misinformation_indicators\n",
    "        )\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def detect_potential_misinformation_indicators(self, text):\n",
    "        # Placeholder for misinformation detection logic\n",
    "        # This could use keyword matching, sentiment analysis, etc.\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "corpus_collector = ClimateArticleCorpus(year_range=(2015, 2024))\n",
    "climate_corpus = corpus_collector.collect_corpus()\n",
    "\n",
    "# Save for later analysis\n",
    "climate_corpus.to_csv('climate_change_corpus.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/scholarly/_scholarly.py:312: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  m = re.search(\"cites=[\\d+,]*\", object[\"citedby_url\"])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m    126\u001b[0m corpus_collector \u001b[38;5;241m=\u001b[39m ClimateArticleCorpus(year_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2015\u001b[39m, \u001b[38;5;241m2024\u001b[39m))\n\u001b[0;32m--> 127\u001b[0m climate_corpus \u001b[38;5;241m=\u001b[39m corpus_collector\u001b[38;5;241m.\u001b[39mcollect_corpus()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Save for later analysis\u001b[39;00m\n\u001b[1;32m    130\u001b[0m climate_corpus\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_change_corpus.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[33], line 103\u001b[0m, in \u001b[0;36mClimateArticleCorpus.collect_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_corpus\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Collect academic and journalistic sources\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     academic_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macademic_scraper\u001b[38;5;241m.\u001b[39mcollect_academic_corpus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear_range)\n\u001b[1;32m    104\u001b[0m     media_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_scraper\u001b[38;5;241m.\u001b[39mscrape_articles(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear_range)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Combine and clean\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 64\u001b[0m, in \u001b[0;36mAcademicScraper.collect_academic_corpus\u001b[0;34m(self, year_range)\u001b[0m\n\u001b[1;32m     61\u001b[0m corpus \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Directly call scraping methods\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m scholar_pubs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_google_scholar(year_range)\n\u001b[1;32m     65\u001b[0m arxiv_pubs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_arxiv(year_range)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Combine and convert to DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m, in \u001b[0;36mAcademicScraper.scrape_google_scholar\u001b[0;34m(self, year_range)\u001b[0m\n\u001b[1;32m     14\u001b[0m search_query \u001b[38;5;241m=\u001b[39m scholarly\u001b[38;5;241m.\u001b[39msearch_pubs(term)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pub \u001b[38;5;129;01min\u001b[39;00m search_query:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Fill the publication details\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     pub \u001b[38;5;241m=\u001b[39m scholarly\u001b[38;5;241m.\u001b[39mfill(pub)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbib\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pub:\n\u001b[1;32m     19\u001b[0m         pub_bib \u001b[38;5;241m=\u001b[39m pub[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbib\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_scholarly.py:238\u001b[0m, in \u001b[0;36m_Scholarly.fill\u001b[0;34m(self, object, sections, sortby, publication_limit)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontainer_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPublication\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    237\u001b[0m     publication_parser \u001b[38;5;241m=\u001b[39m PublicationParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__nav)\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m publication_parser\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/publication_parser.py:368\u001b[0m, in \u001b[0;36mPublicationParser.fill\u001b[0;34m(self, publication)\u001b[0m\n\u001b[1;32m    366\u001b[0m     publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilled\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m PublicationSource\u001b[38;5;241m.\u001b[39mPUBLICATION_SEARCH_SNIPPET:\n\u001b[0;32m--> 368\u001b[0m     bibtex_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bibtex(publication[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl_scholarbib\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    369\u001b[0m     bibtex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnav\u001b[38;5;241m.\u001b[39m_get_page(bibtex_url)\n\u001b[1;32m    370\u001b[0m     parser \u001b[38;5;241m=\u001b[39m bibtexparser\u001b[38;5;241m.\u001b[39mbparser\u001b[38;5;241m.\u001b[39mBibTexParser(common_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/publication_parser.py:413\u001b[0m, in \u001b[0;36mPublicationParser._get_bibtex\u001b[0;34m(self, bib_url)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_bibtex\u001b[39m(\u001b[38;5;28mself\u001b[39m, bib_url) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    411\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the bibtex url\"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnav\u001b[38;5;241m.\u001b[39m_get_soup(bib_url)\n\u001b[1;32m    414\u001b[0m     styles \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs_citi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m styles:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_page(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://scholar.google.com\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url))\n\u001b[1;32m    240\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     res \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_navigator.py:132\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_captcha:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot a captcha request.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m     session \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39m_handle_captcha2(pagerequest)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Retry request within same session\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_proxy_generator.py:422\u001b[0m, in \u001b[0;36mProxyGenerator._handle_captcha2\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     cur \u001b[38;5;241m=\u001b[39m cur \u001b[38;5;241m+\u001b[39m log_interval \u001b[38;5;66;03m# Update before exceptions can happen\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     WebDriverWait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_webdriver(), log_interval)\u001b[38;5;241m.\u001b[39muntil_not(\u001b[38;5;28;01mlambda\u001b[39;00m drv : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_webdriver_has_captcha())\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:127\u001b[0m, in \u001b[0;36mWebDriverWait.until_not\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly\n",
    "\n",
    "class AcademicScraper:\n",
    "    def __init__(self, search_terms=['climate change', 'global warming']):\n",
    "        self.search_terms = search_terms\n",
    "\n",
    "    def scrape_google_scholar(self, year_range=(2010, 2024)):\n",
    "        publications = []\n",
    "        for term in self.search_terms:\n",
    "            try:\n",
    "                search_query = scholarly.search_pubs(term)\n",
    "                for pub in search_query:\n",
    "                    # Fill the publication details\n",
    "                    pub = scholarly.fill(pub)\n",
    "                    if 'bib' in pub:\n",
    "                        pub_bib = pub['bib']\n",
    "                        pub_year = pub_bib.get('year', 0)\n",
    "                        try:\n",
    "                            pub_year = int(pub_year)\n",
    "                        except (ValueError, TypeError):\n",
    "                            pub_year = 0\n",
    "                        if year_range[0] <= pub_year <= year_range[1]:\n",
    "                            publications.append({\n",
    "                                'title': pub_bib.get('title', ''),\n",
    "                                'author': pub_bib.get('author', ''),\n",
    "                                'year': pub_year,\n",
    "                                'text': pub_bib.get('abstract', ''),\n",
    "                                'source': 'Google Scholar'\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Google Scholar search for term {term}: {e}\")\n",
    "        \n",
    "        return publications\n",
    "\n",
    "    def scrape_arxiv(self, year_range=(2010, 2024)):\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        publications = []\n",
    "        \n",
    "        for term in self.search_terms:\n",
    "            query = f'search_query=all:{term}&start=0&max_results=100'\n",
    "            response = requests.get(base_url + query)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                for entry in soup.find_all('entry'):\n",
    "                    published_year = int(entry.published.text[:4])\n",
    "                    if year_range[0] <= published_year <= year_range[1]:\n",
    "                        publications.append({\n",
    "                            'title': entry.title.text,\n",
    "                            'author': ', '.join([au.text for au in entry.find_all('author')]),\n",
    "                            'year': published_year,\n",
    "                            'text': entry.summary.text,\n",
    "                            'source': 'arXiv'\n",
    "                        })\n",
    "        return publications\n",
    "\n",
    "    def collect_academic_corpus(self, year_range=(2010, 2024)):\n",
    "        corpus = []\n",
    "        \n",
    "        # Directly call scraping methods\n",
    "        scholar_pubs = self.scrape_google_scholar(year_range)\n",
    "        arxiv_pubs = self.scrape_arxiv(year_range)\n",
    "        \n",
    "        # Combine and convert to DataFrame\n",
    "        corpus.extend(scholar_pubs)\n",
    "        corpus.extend(arxiv_pubs)\n",
    "        \n",
    "        return pd.DataFrame(corpus)\n",
    "\n",
    "class MediaScraper:\n",
    "    def __init__(self, sources=None):\n",
    "        self.sources = sources or {\n",
    "            'news_outlets': [\n",
    "                'https://www.nytimes.com/section/climate',\n",
    "                'https://www.theguardian.com/environment/climate-change',\n",
    "                'https://www.reuters.com/sustainability/',\n",
    "                'https://www.bbc.com/news/science-environment'\n",
    "            ],\n",
    "            'science_media': [\n",
    "                'https://www.nature.com/climate-change',\n",
    "                'https://www.scientificamerican.com/environment/',\n",
    "                'https://www.newscientist.com/subject/environment/'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def scrape_articles(self, year_range):\n",
    "        # Placeholder implementation as scraping news sites requires proper parsing and rate limiting\n",
    "        # This function should be implemented using requests, BeautifulSoup, or newspaper3k where applicable\n",
    "        # Right now, returning an empty list as a placeholder\n",
    "        return pd.DataFrame(columns=['title', 'author', 'year', 'text', 'source'])\n",
    "\n",
    "class ClimateArticleCorpus:\n",
    "    def __init__(self, year_range=(2015, 2024)):\n",
    "        self.year_range = year_range\n",
    "        self.academic_scraper = AcademicScraper()\n",
    "        self.media_scraper = MediaScraper()\n",
    "    \n",
    "    def collect_corpus(self):\n",
    "        # Collect academic and journalistic sources\n",
    "        academic_df = self.academic_scraper.collect_academic_corpus(self.year_range)\n",
    "        media_df = self.media_scraper.scrape_articles(self.year_range)\n",
    "        \n",
    "        # Combine and clean\n",
    "        combined_df = pd.concat([academic_df, media_df], ignore_index=True)\n",
    "        \n",
    "        # Ensure all text columns are strings and handle NaN values\n",
    "        combined_df['text'] = combined_df['text'].fillna('').astype(str)\n",
    "        \n",
    "        # Additional preprocessing\n",
    "        combined_df['word_count'] = combined_df['text'].apply(lambda x: len(str(x).split()))\n",
    "        combined_df['contains_misinformation_indicators'] = combined_df['text'].apply(\n",
    "            self.detect_potential_misinformation_indicators\n",
    "        )\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    def detect_potential_misinformation_indicators(self, text):\n",
    "        # Placeholder for misinformation detection logic\n",
    "        # This could use keyword matching, sentiment analysis, etc.\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "corpus_collector = ClimateArticleCorpus(year_range=(2015, 2024))\n",
    "climate_corpus = corpus_collector.collect_corpus()\n",
    "\n",
    "# Save for later analysis\n",
    "climate_corpus.to_csv('climate_change_corpus.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m     83\u001b[0m corpus_collector \u001b[38;5;241m=\u001b[39m ClimateArticleCorpus(year_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2015\u001b[39m, \u001b[38;5;241m2024\u001b[39m))\n\u001b[0;32m---> 84\u001b[0m climate_corpus \u001b[38;5;241m=\u001b[39m corpus_collector\u001b[38;5;241m.\u001b[39mcollect_corpus()\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Save for later analysis\u001b[39;00m\n\u001b[1;32m     87\u001b[0m climate_corpus\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_change_corpus.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[33], line 103\u001b[0m, in \u001b[0;36mClimateArticleCorpus.collect_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_corpus\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Collect academic and journalistic sources\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     academic_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macademic_scraper\u001b[38;5;241m.\u001b[39mcollect_academic_corpus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear_range)\n\u001b[1;32m    104\u001b[0m     media_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_scraper\u001b[38;5;241m.\u001b[39mscrape_articles(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear_range)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Combine and clean\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 73\u001b[0m, in \u001b[0;36mAcademicScraper.collect_academic_corpus\u001b[0;34m(self, year_range)\u001b[0m\n\u001b[1;32m     70\u001b[0m corpus \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Directly call scraping methods\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m scholar_pubs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_google_scholar(year_range)\n\u001b[1;32m     74\u001b[0m arxiv_pubs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscrape_arxiv(year_range)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Combine and convert to DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 15\u001b[0m, in \u001b[0;36mAcademicScraper.scrape_google_scholar\u001b[0;34m(self, year_range, max_results)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_terms:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m         search_query \u001b[38;5;241m=\u001b[39m scholarly\u001b[38;5;241m.\u001b[39msearch_pubs(term)\n\u001b[1;32m     16\u001b[0m         count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pub \u001b[38;5;129;01min\u001b[39;00m search_query:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_scholarly.py:160\u001b[0m, in \u001b[0;36m_Scholarly.search_pubs\u001b[0;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Searches by query and returns a generator of Publication objects\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m:param query: terms to be searched\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_url(_PUBSEARCH\u001b[38;5;241m.\u001b[39mformat(requests\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mquote(query)), patents\u001b[38;5;241m=\u001b[39mpatents,\n\u001b[1;32m    158\u001b[0m                           citations\u001b[38;5;241m=\u001b[39mcitations, year_low\u001b[38;5;241m=\u001b[39myear_low, year_high\u001b[38;5;241m=\u001b[39myear_high,\n\u001b[1;32m    159\u001b[0m                           sort_by\u001b[38;5;241m=\u001b[39msort_by, include_last_year\u001b[38;5;241m=\u001b[39minclude_last_year, start_index\u001b[38;5;241m=\u001b[39mstart_index)\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__nav\u001b[38;5;241m.\u001b[39msearch_publications(url)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_navigator.py:296\u001b[0m, in \u001b[0;36mNavigator.search_publications\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_publications\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _SearchScholarIterator:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a Publication Generator given a url\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    :param url: the url where publications can be found.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    :rtype: {_SearchScholarIterator}\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _SearchScholarIterator(\u001b[38;5;28mself\u001b[39m, url)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/publication_parser.py:53\u001b[0m, in \u001b[0;36m_SearchScholarIterator.__init__\u001b[0;34m(self, nav, url)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pubtype \u001b[38;5;241m=\u001b[39m PublicationSource\u001b[38;5;241m.\u001b[39mPUBLICATION_SEARCH_SNIPPET \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scholar?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url \u001b[38;5;28;01melse\u001b[39;00m PublicationSource\u001b[38;5;241m.\u001b[39mJOURNAL_CITATION_LIST\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav \u001b[38;5;241m=\u001b[39m nav\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_url(url)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_total_results()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_parser \u001b[38;5;241m=\u001b[39m PublicationParser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/publication_parser.py:59\u001b[0m, in \u001b[0;36m_SearchScholarIterator._load_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_url\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# this is temporary until setup json file\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nav\u001b[38;5;241m.\u001b[39m_get_soup(url)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs_r gs_or gs_scl\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgsc_mpat_ttl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_soup\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BeautifulSoup:\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     html \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_page(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://scholar.google.com\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url))\n\u001b[1;32m    240\u001b[0m     html \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xa0\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     res \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_navigator.py:132\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[0;34m(self, pagerequest, premium)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m has_captcha:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot a captcha request.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 132\u001b[0m     session \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39m_handle_captcha2(pagerequest)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Retry request within same session\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scholarly/_proxy_generator.py:422\u001b[0m, in \u001b[0;36mProxyGenerator._handle_captcha2\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     cur \u001b[38;5;241m=\u001b[39m cur \u001b[38;5;241m+\u001b[39m log_interval \u001b[38;5;66;03m# Update before exceptions can happen\u001b[39;00m\n\u001b[0;32m--> 422\u001b[0m     WebDriverWait(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_webdriver(), log_interval)\u001b[38;5;241m.\u001b[39muntil_not(\u001b[38;5;28;01mlambda\u001b[39;00m drv : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_webdriver_has_captcha())\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/selenium/webdriver/support/wait.py:127\u001b[0m, in \u001b[0;36mWebDriverWait.until_not\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from scholarly import scholarly\n",
    "import time\n",
    "\n",
    "class AcademicScraper:\n",
    "    def __init__(self, search_terms=['climate change', 'global warming']):\n",
    "        self.search_terms = search_terms\n",
    "\n",
    "    def scrape_google_scholar(self, year_range=(2010, 2024), max_results=20):\n",
    "        publications = []\n",
    "        for term in self.search_terms:\n",
    "            try:\n",
    "                search_query = scholarly.search_pubs(term)\n",
    "                count = 0\n",
    "                for pub in search_query:\n",
    "                    if count >= max_results:\n",
    "                        break\n",
    "\n",
    "                    # Add delay to prevent getting blocked\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    # Fill the publication details\n",
    "                    pub = scholarly.fill(pub)\n",
    "                    if 'bib' in pub:\n",
    "                        pub_bib = pub['bib']\n",
    "                        pub_year = pub_bib.get('year', 0)\n",
    "                        try:\n",
    "                            pub_year = int(pub_year)\n",
    "                        except (ValueError, TypeError):\n",
    "                            pub_year = 0\n",
    "                        if year_range[0] <= pub_year <= year_range[1]:\n",
    "                            publications.append({\n",
    "                                'title': pub_bib.get('title', ''),\n",
    "                                'author': pub_bib.get('author', ''),\n",
    "                                'year': pub_year,\n",
    "                                'text': pub_bib.get('abstract', ''),\n",
    "                                'source': 'Google Scholar'\n",
    "                            })\n",
    "                    count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Google Scholar search for term {term}: {e}\")\n",
    "        \n",
    "        return publications\n",
    "\n",
    "    def scrape_arxiv(self, year_range=(2010, 2024)):\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        publications = []\n",
    "        \n",
    "        for term in self.search_terms:\n",
    "            query = f'search_query=all:{term}&start=0&max_results=100'\n",
    "            response = requests.get(base_url + query)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'xml')\n",
    "                for entry in soup.find_all('entry'):\n",
    "                    published_year = int(entry.published.text[:4])\n",
    "                    if year_range[0] <= published_year <= year_range[1]:\n",
    "                        publications.append({\n",
    "                            'title': entry.title.text,\n",
    "                            'author': ', '.join([au.text for au in entry.find_all('author')]),\n",
    "                            'year': published_year,\n",
    "                            'text': entry.summary.text,\n",
    "                            'source': 'arXiv'\n",
    "                        })\n",
    "        return publications\n",
    "\n",
    "    def collect_academic_corpus(self, year_range=(2010, 2024)):\n",
    "        corpus = []\n",
    "        \n",
    "        # Directly call scraping methods\n",
    "        scholar_pubs = self.scrape_google_scholar(year_range)\n",
    "        arxiv_pubs = self.scrape_arxiv(year_range)\n",
    "        \n",
    "        # Combine and convert to DataFrame\n",
    "        corpus.extend(scholar_pubs)\n",
    "        corpus.extend(arxiv_pubs)\n",
    "        \n",
    "        return pd.DataFrame(corpus)\n",
    "\n",
    "# Usage\n",
    "corpus_collector = ClimateArticleCorpus(year_range=(2015, 2024))\n",
    "climate_corpus = corpus_collector.collect_corpus()\n",
    "\n",
    "# Save for later analysis\n",
    "climate_corpus.to_csv('climate_change_corpus.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "class MediaScraper:\n",
    "    def __init__(self, sources=None):\n",
    "        self.sources = sources or {\n",
    "            'news_outlets': [\n",
    "                'https://www.nytimes.com/section/climate',\n",
    "                'https://www.theguardian.com/environment/climate-change',\n",
    "                'https://www.reuters.com/sustainability/',\n",
    "                'https://www.bbc.com/news/science-environment'\n",
    "            ],\n",
    "            'science_media': [\n",
    "                'https://www.nature.com/climate-change',\n",
    "                'https://www.scientificamerican.com/environment/',\n",
    "                'https://www.newscientist.com/subject/environment/'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def scrape_articles(self, year_range):\n",
    "        articles = []\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        \n",
    "        for category, urls in self.sources.items():\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        articles_list = soup.find_all('article')\n",
    "                        \n",
    "                        for article in articles_list:\n",
    "                            title_tag = article.find(['h1', 'h2', 'h3'])\n",
    "                            title = title_tag.text.strip() if title_tag else 'No Title'\n",
    "                            author_tag = article.find('span', class_='byline')\n",
    "                            author = author_tag.text.strip() if author_tag else 'Unknown'\n",
    "                            year = 2024  # Placeholder year, this should be extracted based on article metadata if available\n",
    "                            text_tag = article.find('p')\n",
    "                            text = text_tag.text.strip() if text_tag else 'No Content'\n",
    "                            \n",
    "                            # Append article details if within the year range\n",
    "                            if year_range[0] <= year <= year_range[1]:\n",
    "                                articles.append({\n",
    "                                    'title': title,\n",
    "                                    'author': author,\n",
    "                                    'year': year,\n",
    "                                    'text': text,\n",
    "                                    'source': url\n",
    "                                })\n",
    "                    \n",
    "                    # Add a delay to prevent overwhelming the server\n",
    "                    time.sleep(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping {url}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(articles)\n",
    "\n",
    "# Usage\n",
    "media_scraper = MediaScraper()\n",
    "media_articles = media_scraper.scrape_articles(year_range=(2015, 2024))\n",
    "\n",
    "# Save for later analysis\n",
    "media_articles.to_csv('media_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "class MediaScraper:\n",
    "    def __init__(self, sources=None):\n",
    "        self.sources = sources or {\n",
    "            'news_outlets': [\n",
    "                'https://www.nytimes.com/section/climate',\n",
    "                'https://www.theguardian.com/environment/climate-change',\n",
    "                'https://www.reuters.com/sustainability/',\n",
    "                'https://www.bbc.com/news/science-environment'\n",
    "            ],\n",
    "            'science_media': [\n",
    "                'https://www.nature.com/climate-change',\n",
    "                'https://www.scientificamerican.com/environment/',\n",
    "                'https://www.newscientist.com/subject/environment/'\n",
    "            ]\n",
    "        }\n",
    "        self.scraped_urls = set()  # Track scraped URLs to ensure uniqueness\n",
    "    \n",
    "    def scrape_articles(self, year_range, max_articles_per_source=50):\n",
    "        articles = []\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        \n",
    "        for category, urls in self.sources.items():\n",
    "            for url in urls:\n",
    "                try:\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        articles_list = soup.find_all('article')\n",
    "                        count = 0\n",
    "                        \n",
    "                        for article in articles_list:\n",
    "                            if count >= max_articles_per_source:\n",
    "                                break\n",
    "\n",
    "                            link_tag = article.find('a', href=True)\n",
    "                            if link_tag:\n",
    "                                article_url = urljoin(url, link_tag['href'])\n",
    "                                if article_url in self.scraped_urls:\n",
    "                                    continue\n",
    "\n",
    "                                self.scraped_urls.add(article_url)\n",
    "                                article_response = requests.get(article_url, headers=headers)\n",
    "                                if article_response.status_code == 200:\n",
    "                                    article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "                                    \n",
    "                                    title_tag = article_soup.find(['h1', 'h2', 'h3'])\n",
    "                                    title = title_tag.text.strip() if title_tag else 'No Title'\n",
    "                                    \n",
    "                                    author_tag = article_soup.find('span', class_='byline') or article_soup.find('meta', {'name': 'author'})\n",
    "                                    author = author_tag['content'].strip() if author_tag and author_tag.has_attr('content') else (author_tag.text.strip() if author_tag else 'Unknown')\n",
    "                                    \n",
    "                                    date_tag = article_soup.find('time') or article_soup.find('meta', {'property': 'article:published_time'})\n",
    "                                    year = 2024  # Default placeholder year\n",
    "                                    if date_tag:\n",
    "                                        if date_tag.has_attr('datetime'):\n",
    "                                            year = int(date_tag['datetime'][:4])\n",
    "                                        elif date_tag.text.strip():\n",
    "                                            year = int(date_tag.text.strip()[-4:])\n",
    "                                    \n",
    "                                    # Append article details if within the year range\n",
    "                                    if year_range[0] <= year <= year_range[1]:\n",
    "                                        articles.append({\n",
    "                                            'title': title,\n",
    "                                            'author': author,\n",
    "                                            'year': year,\n",
    "                                            'source': article_url,\n",
    "                                            'category': category\n",
    "                                        })\n",
    "                                        count += 1\n",
    "                    \n",
    "                    # Add a delay to prevent overwhelming the server\n",
    "                    time.sleep(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping {url}: {e}\")\n",
    "        \n",
    "        return pd.DataFrame(articles)\n",
    "\n",
    "# Usage\n",
    "media_scraper = MediaScraper()\n",
    "media_articles = media_scraper.scrape_articles(year_range=(2015, 2024))\n",
    "\n",
    "# Save for later analysis\n",
    "media_articles.to_csv('media_articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class AcademicScraper:\n",
    "    def __init__(self, search_terms=['climate change', 'global warming']):\n",
    "        self.search_terms = search_terms\n",
    "\n",
    "    def scrape_arxiv(self, year_range=(1991, 2024), max_results=100):\n",
    "        base_url = 'http://export.arxiv.org/api/query?'\n",
    "        publications = []\n",
    "        seen_titles = set()  # Track seen titles to avoid duplicates\n",
    "        \n",
    "        for term in self.search_terms:\n",
    "            start = 0\n",
    "            while True:\n",
    "                query = f'search_query=all:{term}&start={start}&max_results={max_results}'\n",
    "                try:\n",
    "                    response = requests.get(base_url + query, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        soup = BeautifulSoup(response.content, 'xml')\n",
    "                        entries = soup.find_all('entry')\n",
    "                        if not entries:\n",
    "                            break\n",
    "                        \n",
    "                        for entry in entries:\n",
    "                            title = entry.title.text.strip()\n",
    "                            if title in seen_titles:\n",
    "                                continue\n",
    "                            \n",
    "                            seen_titles.add(title)\n",
    "                            published_year = int(entry.published.text[:4])\n",
    "                            if year_range[0] <= published_year <= year_range[1]:\n",
    "                                publications.append({\n",
    "                                    'title': title,\n",
    "                                    'author': ', '.join([au.text for au in entry.find_all('author')]),\n",
    "                                    'year': published_year,\n",
    "                                    'text': entry.summary.text,\n",
    "                                    'source': 'arXiv'\n",
    "                                })\n",
    "                        \n",
    "                        # Move to the next batch of results\n",
    "                        start += max_results\n",
    "                    else:\n",
    "                        break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error fetching arXiv data for term {term}: {e}\")\n",
    "                    break  # Stop further retries if there's an error\n",
    "        return publications\n",
    "\n",
    "    def collect_academic_corpus(self, year_range=(1991, 2024)):\n",
    "        corpus = []\n",
    "        \n",
    "        # Directly call scraping methods\n",
    "        arxiv_pubs = self.scrape_arxiv(year_range)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        corpus.extend(arxiv_pubs)\n",
    "        \n",
    "        return pd.DataFrame(corpus)\n",
    "\n",
    "# Usage\n",
    "academic_scraper = AcademicScraper()\n",
    "academic_corpus = academic_scraper.collect_academic_corpus(year_range=(1991, 2024))\n",
    "\n",
    "# Save for later analysis\n",
    "academic_corpus.to_csv('academic_corpus.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
